<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AI-Figures: A Fine-grained Task-oriented Dataset for Multimodal Scientific Literature Understanding">
  <meta name="keywords" content="AI-Figures">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AI-Figures: A Fine-grained Task-oriented Dataset for Multimodal Scientific Literature Understanding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AI-Figures: A Fine-grained Task-oriented Dataset for Multimodal Scientific Literature Understanding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=yBXIpkAAAAAJ&hl=en">Avishek Lahiri</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=9fBbj5gAAAAJ&hl=en">Medha Sen</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?hl=en&user=AevNQmMAAAAJ&view_op=list_works&sortby=pubdate">Debarshi Kumar Sanyal</a><sup></sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup></sup>Indian Association for the Cultivation of Science</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://doi.org/10.5281/zenodo.15555419"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>AI-Figures dataset</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <!-- Centered image -->
      <img src="./static/images/AI_figures_page-0001.jpg"
           class="ai_fig_pie"
           alt="Classes in AI_figures"
           style="max-width: 100%; height: auto; margin: 0 auto;"/>

      <!-- Subtitle text -->
      <h2 class="subtitle">
        The class-wise distribution in the human annotated (left) and the distantly-supervised (right)  <span class="dnerf"> AI-figures</span> dataset.
      </h2>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diagrams and figures are a powerful medium of communication in scientific research. There is a recent spark in interest in the development of Machine Learning-driven applications involving scientific figures such as multimodal question answering, multimodal document retrieval, text-to-image generation or image captioning. Challenging tasks in this domain may be dependent only on a specific category of scientific figures. But there are no datasets in prior literature which provide a domain-specific broad classification of scientific figures. 
          </p>
          <p>
            To fill this gap, we introduce AI-Figures a large scale dataset containing scientific figure-caption pairs which are classified into 9 different categories. We create this dataset by leveraging the idea of image segmentation and classification using the YOLO model. Our automated data acquisition pipeline can be implemented on other datasets also in order to classify their figures. We benchmark 6 Large Language Vision models and 5 Large Language models on our dataset for various tasks such as figure captioning, tag classification, text-to-figure generation, multimodal question answering and multimodal document retrieval. We show that there is a significant increase in a model's inference capabilities when we finetune it on our dataset.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- AI_Figures pipeline -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Our Novel Pipeline for Figure Extaction</h2>
        <div class="publication-video">
          <img src="./static/images/Figure_extraction_pipeline.png"
              class="ai_fig_pipeline"
              alt="Classes in AI_figures"
              style="max-width: 100%; height: auto; margin: 0 auto;"/>
                <!-- Subtitle text -->
          <h2 class="subtitle">
            The construction pipeline used to curate the AI-Figures dataset.
          </h2>
        </div>
        <div class="content has-text-justified">
          <p>
            Our dataset construction process is built on the insight that, for any given page of a research paper, it is sufficient to segment the region containing the figure along with its nearest caption text. To achieve this, we employ an object detection pipeline. Each PDF page is first converted into an image using the pdf2image library. We then manually annotated figure-caption pairs from 200 research papers, 100 each from ACL Anthology and CVPR respectively, to train a YOLO-based object detection model. The Roboflow Annotate platform was used by to label bounding boxes for the figure and caption regions for each document page and subsequently assign each figure to one of the pre-defined class categories. 
          </p>
        </div>
        <div class="publication-video">
            <img src="./static/images/roboflow.png"
                class="roboflow"
                alt="Roboflow Annotate"
                style="max-width: 100%; height: auto; margin: 0 auto;"/>
                <!-- Subtitle text -->
            <h2 class="subtitle">
              The Robflow Annotate platform.
            </h2>
        </div>
        <div class="content has-text-justified">
          <p>
            YOLO (You Only Look Once) is a hugely popular fast object detection and image segmentation model. In YOL, object detection is reformulated as a regression problem from image pixels to bounding box coordinates and class probabilities. We train several versions of YOLO models including YOLOv5s, YOLOv5m, YOLOv5I, YOLOv8s, YOLOv8m and YOLOv8I on our human annotated dataset (AI-FIGURES HUMAN). Based on the mean Average Precision scores on the test set of AI-FIGURES HUMAN, we select YOLOv8m for the creation of the larger AI-FIGURES corpus. This scalable pipeline not only streamlines scientific figure extraction but is also adaptable for use across diverse scholarly domains.
          </p>
        </div>
        <div class="publication-video">
            <img src="./static/images/class_wise.PNG"
                class="roboflow"
                alt="Class wise stats"
                style="max-width: 70%; height: auto; margin: 0 auto;"/>
                <!-- Subtitle text -->
            <h2 class="subtitle">
              Class-wise statistics of AI-Figures-Human and AI-Figures
            </h2>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>
<div class="container is-max-desktop">
  <!-- Abstract. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Human Evaluation of AI-FIGURES</h2>
      <div class="content has-text-justified">
  <p>
    To evaluate the quality of our dataset construction process, we conducted a manual evaluation. Six randomized sets were created, each containing 100 figure-caption-category triplets. Each of the six annotators—graduate students with backgrounds in Computer Science—was provided with the extracted figure, its corresponding caption, the predicted category, and the original paper’s URL.
  </p>
  <p>
    Annotators were instructed to classify each triplet into one of the following four categories:
  </p>
  <ul>
    <li><strong>Acceptable</strong>: The figure is correctly segmented, properly classified, and paired with the correct caption.</li>
    <li><strong>Figure Segmentation Error</strong>: The bounding box around the figure is incorrect or poorly cropped.</li>
    <li><strong>Figure Classification Error</strong>: The figure is assigned to the wrong category.</li>
    <li><strong>Figure-Caption Pairing Error</strong>: The extracted caption does not correspond to the associated figure.</li>
  </ul>
  <p>
    According to the aggregated results, <strong>83.3%</strong> of the samples were deemed Acceptable. The most frequent issue was <em>Figure-Caption Pairing Error</em>, followed by <em>Classification Error</em> and <em>Segmentation Error</em>, respectively.
  </p>
</div>
        <div class="publication-video">
            <img src="./static/images/Dataset_Human_Eval_page-0001.jpg"
                class="roboflow"
                alt="Human Eval"
                style="max-width: 70%; height: auto; margin: 0 auto;"/>
                <!-- Subtitle text -->
            <h2 class="subtitle">
              Results for the manual analysis of our distantly supervised dataset, AI-Figures
            </h2>
        </div>
    </div>
  </div>
</div>
</section>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{2025aifigures,
  author    = {Avishek Lahiri and Medha Sen and Debarshi Kumar Sanyal},
  title     = {{AI-Figures: A Fine-grained Task-oriented Dataset for Multimodal Scientific Literature Understanding}},
  booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia (ACMMM ’25)},
  year      = {2025},
  publisher = {ACM},
  note      = {*Equal contribution},
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
            This mwebsite was adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
